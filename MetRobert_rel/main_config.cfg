[args]
# Bert pre-trained model selected in the list [bert-base-cased, roberta-base, albert-base-v1 / albert-large-v1] (default = roberta-base)
##bert_model = /vol/tensusers4/jgrunwald/metaphorclam/metaphorclam/MetRobert_run2/saves/roberta-base/0_20220429-1650
bert_model = ./MetRobert_rel/saves/roberta-base/3_20220822-1301
# The input data dir. Should contain the .tsv files (VUAall / VUAverb / MOH-X/CLS / TroFi/CLS / VUA_DeepMet)
data_dir = data_sample/VUAall_sample
# The name of the task to train (vua(1-fold) / trofi(10-fold))
task_name = vua
# The name of model type (default = MELBERT) (BERT_BASE / BERT_SEQ / MELBERT_SPV / MELBERT_MIP / MELBERT)
model_type = MELBERT
# The hidden dimension for classifier (default = 768)
classifier_hidden = 1168
# Learning rate scheduler (default = warmup_linear) (none / warmup_linear)
lr_schedule = warmup_linear
# Training epochs to perform linear learning rate warmup for. (default = 2)
warmup_epoch = 2
# Dropout ratio (default = 0.2)
drop_ratio = 0.2
# K-fold (default = 10)
kfold = 10
# Number of bagging (default = 0) normal is 10  (0 not for using bagging technique)
num_bagging = 0 
# The index of bagging only for the case using bagging technique (default = 0)
bagging_index = 0
# Use additional linguistic features POS tag (default = True)
use_pos = True
# Local context (default = True)
use_local_context= True
# The maximum total input sequence length after WordPiece tokenization. (default = 200) (150 scores better then 200)
max_seq_length = 150
# Whether to run training (default = False)
do_train = False
# Whether to run eval on the test set (default = False)
do_test = False
# Whether to run eval on the dev set. (default = False)
do_eval = False
# Whether to create preds with the save file (default = False)
do_pred = True
# Set this flag if you are using an uncased model. (default = False)
do_lower_case = False
# Weight of metaphor. (default = 3.0)
class_weight = 3
# Total batch size for training. (default = 32)
train_batch_size = 32
# Total batch size for eval. (default = 8)
eval_batch_size = 8
# The initial learning rate for Adam (default = 3e-5)
learning_rate = 8.979575521657289e-05 
# Total number of training epochs to perform. (default = 3.0)
num_train_epoch = 3
# Whether not to use CUDA when available (default = False)
no_cuda = False
# random seed for initialization (default = 42)
seed = 42
